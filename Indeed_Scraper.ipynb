{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Variables\"\"\"\n",
    "prefix_url = \"http://ca.indeed.com\"\n",
    "folder_name = \"Job-Posts\"\n",
    "token = \"Indeed.com\"\n",
    "created = False\n",
    "list_of_urls = []\n",
    "list_of_links = []\n",
    "list_of_banks = [\"RBC\", \"Scotiabank\", \"CIBC\", \"BMO Financial Group\", \"TD Bank\"]\n",
    "list_of_links.append(\"http://ca.indeed.com/jobs?q=data+science&l=Toronto%2C+ON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.secs = self.end - self.start\n",
    "        self.msecs = self.secs * 1000  # millisecs\n",
    "        if self.verbose:\n",
    "            print 'elapsed time: %f ms' % self.msecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def parse_next_url(url):\n",
    "    response = urllib2.urlopen(url)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #print url\n",
    "    #print soup.find(\"title\")\n",
    "    return soup\n",
    "\n",
    "def make_folder(directory):\n",
    "    directory = directory + \"-\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    directory = os.path.join(os.getcwd(), directory)\n",
    "    os.makedirs(directory)\n",
    "        \n",
    "def save_job_description(info, text, url):\n",
    "    for key, val in info.iteritems():\n",
    "        file_name = ((key + \"-\" + val).replace(\" / \", \"_\").replace(\"/\", \"_\").replace(\" \", \"_\")) + \".txt\"\n",
    "        text_file = open(folder_name + \"/\" + file_name, \"w+\")\n",
    "        text_file.write(url.encode('utf-8'))\n",
    "        text_file.write(text)\n",
    "        text_file.close()\n",
    "    \n",
    "def parse_text(soup, type_of_company, title, url):\n",
    "\n",
    "    job_info = {}\n",
    "    complete_text = \"\"\n",
    "    indeed = is_it_indeed(soup)\n",
    "    \n",
    "    if indeed:\n",
    "        job = soup.select('#job_summary')\n",
    "        raw_text = job[0].get_text().encode('utf-8')\n",
    "        complete_text = raw_text.strip()\n",
    "        \n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "        \n",
    "    if type_of_company == \"RBC\":\n",
    "        # Grabbing the raw job description text\n",
    "        job = soup.select(\".job\") # Entire job container\n",
    "        raw_text = job[0].get_text()\n",
    "        raw_text = raw_text.replace(u'\\xa0', u' ').encode('utf-8')\n",
    "        complete_text = raw_text.strip()\n",
    "        \n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "                \n",
    "    if type_of_company == \"Scotiabank\":\n",
    "        job = soup.select(\"#jobDesc p\") # Entire job container\n",
    "        if job:\n",
    "            for item in job:\n",
    "                complete_text += item.get_text()\n",
    "        else:\n",
    "            job = soup.select(\"#descriptiontext\") # Entire job container\n",
    "            if job:\n",
    "                for item in job:\n",
    "                    complete_text += item.get_text()\n",
    "            else:\n",
    "                job = soup.select(\"#requirementstext\") # Entire job container\n",
    "                for item in job:\n",
    "                    complete_text += item.get_text()\n",
    "                \n",
    "        complete_text = complete_text.encode('utf-8')\n",
    "        \n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "            \n",
    "    \n",
    "    if type_of_company == \"BMO Financial Group\":\n",
    "        job = soup.select('#initialHistory')\n",
    "        #print soup.prettify()\n",
    "        #print len(job)\n",
    "        complete_text = job[0]['value'].encode('utf-8')\n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "        \n",
    "    if type_of_company == \"TD Bank\":\n",
    "        job = soup.select(\"#tblJobDetail tr\")\n",
    "        #print len(job)\n",
    "        for a in job:\n",
    "            iteration = 0\n",
    "            row = a.select(\"td\")\n",
    "            for b in row:\n",
    "                if iteration == 0:\n",
    "                    complete_text += (b.get_text().strip() + \": \")\n",
    "                if iteration == 1:\n",
    "                    complete_text += b.get_text().strip()\n",
    "                iteration = iteration + 1\n",
    "            complete_text += \"\\n\"\n",
    "        complete_text = complete_text.encode('utf-8')\n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "    if type_of_company == \"CIBC\":\n",
    "        job = soup.select('#initialHistory')\n",
    "        complete_text = job[0]['value'].encode('utf-8')\n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "    \n",
    "def create_paginations():\n",
    "    for i in range(190, 0, -10):\n",
    "        link = \"http://ca.indeed.com/jobs?q=data+science&l=Toronto%2C+ON&start=\" + str(i)\n",
    "        list_of_links.append(link)\n",
    "        \n",
    "def is_it_indeed(soup):\n",
    "    site_title = soup.find(\"title\").text\n",
    "    True if token in site_title else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parsing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing http://ca.indeed.com/jobs?q=data+science&l=Toronto%2C+ON&start=30 from the list of links to scrape\n",
      "There are 14 job postings in this page\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0e4904f48be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_next_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mparse_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'company'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" jobs were saved!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-140beeb8ed02>\u001b[0m in \u001b[0;36mparse_text\u001b[0;34m(soup, type_of_company, title, url)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mjob_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcomplete_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mindeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_it_indeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-140beeb8ed02>\u001b[0m in \u001b[0;36mis_it_indeed\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_it_indeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0msite_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msite_title\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    make_folder(folder_name)\n",
    "    while list_of_links:\n",
    "        current_link = list_of_links.pop()\n",
    "        print \"Removing \" + current_link + \" from the list of links to scrape\"\n",
    "\n",
    "        soup = parse_next_url(current_link)\n",
    "\n",
    "        if not created:\n",
    "            create_paginations()\n",
    "            created = True\n",
    "\n",
    "        job_entry = soup.findAll(\"div\", class_=\"row\")\n",
    "\n",
    "        print \"There are \" + str(len(job_entry)) + \" job postings in this page\"\n",
    "\n",
    "        for job in job_entry:\n",
    "\n",
    "            company = job.find(\"span\", class_=\"company\")\n",
    "            job_info = job.find(\"h2\", class_=\"jobtitle\")\n",
    "            sponsored = job.find(\"span\", class_=\"sdn\")\n",
    "\n",
    "            #print job\n",
    "\n",
    "            if job_info is None: \n",
    "                # Sponsored\n",
    "                job_descr = job.find(\"a\", class_=\"jobtitle turnstileLink\")\n",
    "            else: \n",
    "                # Normal\n",
    "                job_descr = job_info.find(\"a\", class_=\"turnstileLink\")\n",
    "                \n",
    "#             print \"----\"\n",
    "#             print job_descr\n",
    "            \n",
    "            company_name = company.text.strip()\n",
    "\n",
    "            company_info = company.find(\"a\", class_=\"turnstileLink\")\n",
    "\n",
    "            job_title = job_descr.get(\"title\").strip()\n",
    "            job_href = job_descr.get(\"href\").strip()\n",
    "\n",
    "            final_url = prefix_url + job_href + \"\\n\"\n",
    "\n",
    "    #         if sponsored:\n",
    "    #             print \"Sponsored\"\n",
    "    #         print job_title\n",
    "    #         print company_name\n",
    "    #         print final_url\n",
    "\n",
    "            # Add the url to a list for future crawl\n",
    "            list_of_urls.append({ 'company': company_name.encode('utf-8'), 'title': job_title.encode('utf-8'), 'url': final_url.encode('utf-8') })\n",
    "\n",
    "        for item in list_of_urls:\n",
    "            soup = parse_next_url(item['url'])\n",
    "            parse_text(soup, item['company'], item['title'], item['url'])\n",
    "            \n",
    "print times + \" jobs were saved!\"\n",
    "print \"=> elasped code: %s s\" % t.secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'http', u':', u'//ca.indeed.com/rc/clk', u'?', u'jk=2ac84fc44fb6620c', u'&', u'fccid=537b899e30af3338']\n",
      "[u'What', u'is', u'the', u'opportunity', u'?']\n",
      "[u'Analyze', u',', u'design', u'and', u'implement', u'data', u'science', u'solutions', u'using', u'RBC\\u2019s', u'enterprise', u'suite', u'of', u'analytics', u'tools', u'.', u'Data', u'Science', u'allows', u'us', u'to', u'better', u'understand', u'the', u'implications', u'of', u'what', u'information', u'means', u',', u'identify', u'trends', u',', u'anticipate', u'future', u'behaviors', u',', u'perform', u'pattern', u'matching', u',', u'discover', u'the', u'golden', u'path', u'clients', u'take', u'that', u'leads', u'to', u'a', u'future', u'decision', u'and', u'predict', u'the', u'outcomes', u'of', u'future', u'events', u'.', u'The', u'group', u'specializes', u'in', u'taking', u'full', u'advantage', u'of', u'large', u'data', u'sets', u'to', u'explore', u'and', u'discover', u'new', u'insights', u'that', u'would', u'have', u'not', u'been', u'possible', u'with', u'traditional', u'analytics', u'.', u'Leveraging', u'leading', u'edge', u'technologies', u'and', u'capabilities', u',', u'the', u'group', u'applies', u'machine', u'learning', u'and', u'statistical', u'modelling', u'techniques', u'to', u'help', u'RBC', u'understand', u'the', u'changing', u'business', u'environment', u',', u'discover', u'new', u'growth', u'opportunities', u'and', u'determine', u'where', u'business', u'improvements', u'can', u'be', u'made', u'.', u'The', u'group', u'is', u'equipped', u'with', u'capabilities', u'in', u'text', u'analytics/Natural', u'Language', u'Processing', u',', u'social', u'media', u'analytics', u',', u'Big', u'Data', u'advanced', u'analytics', u'and', u'Machine', u'Learning', u'.']\n",
      "[]\n",
      "[u'What', u'will', u'you', u'do', u'?']\n",
      "[u'Data', u'scientists', u'analyze', u'data', u'using', u'statistical', u',', u'machine', u'learning', u',', u'and', u'analytics', u'techniques', u'to', u'solve', u'complex', u'problems', u',', u'discover', u'insights', u'and', u'identify', u'business', u'opportunities', u'.']\n",
      "[]\n",
      "[u'Statistics', u'&', u'Machine', u'Learning', u':', u'Build', u'statistical', u'and', u'machine', u'learning', u'models', u'to', u'test', u'business', u'hypotheses', u'.']\n",
      "[]\n",
      "[u'Technology', u':', u'Prepare', u'and', u'integrate', u'large', u'and', u'varied', u'datasets', u',', u'build', u'statistical', u'and', u'machine', u'learning', u'models', u'using', u'Python', u'and', u'R', u',', u'and', u'deploy', u'proof', u'of', u'concept', u'solutions', u'.']\n",
      "[]\n",
      "[u'Domain', u'Knowledge', u':', u'Collaborate', u'with', u'business', u'partners', u'to', u'produce', u'insights', u'from', u'business-defined', u'requirements', u'and', u'hypotheses', u'.']\n",
      "[]\n",
      "[u'Communication', u'&', u'Visualization', u':', u'Effectively', u'communicate', u'findings', u'to', u'business', u'partners', u'.', u'Leverage', u'advanced', u'visualization', u'tools', u'(', u'D3', u',', u'R', u',', u'LaTeX', u')', u'to', u'create', u'powerful', u'representations', u'of', u'results', u'.']\n",
      "[]\n",
      "[u'What', u'do', u'you', u'need', u'to', u'succeed', u'?']\n",
      "[u'Must-have']\n",
      "[u'Experience', u'with', u'big', u'data', u'technologies', u'(', u'Hadoop', u'(', u'Pig', u',', u'Hive', u')', u',', u'noSQL/SQL', u'databases', u',', u'parallel', u'processing', u'techniques', u',', u'Apache', u'Spark', u')', u'and', u'statistical', u'methods', u'.']\n",
      "[u'Strong', u'data', u'profiling', u',', u'cleaning', u',', u'and', u'mining', u'.']\n",
      "[u'Ability', u'to', u'perform', u'complex', u'data', u'analysis', u'on', u'large', u'volumes', u'of', u'data', u'and', u'present', u'findings', u'to', u'stakeholders', u'.']\n",
      "[u'Strong', u'knowledge', u'of', u'design', u',', u'development', u',', u'and', u'implementation', u'experience', u'utilizing', u'data', u'science', u'technologies', u'.']\n",
      "[u'Excellent', u'analytical', u',', u'problem', u'solving', u'skills', u'and', u'technical', u'documentation', u'skills', u'.']\n",
      "[u'Familiar', u'with', u'a', u'range', u'of', u'data', u'tools', u'and', u'machine', u'learning', u'techniques', u'.']\n",
      "[u'Programming', u'skills', u'and', u'some', u'knowledge', u'of', u'Python', u',', u'R', u',', u'Java', u'and', u'SQL']\n",
      "[u'Familiar', u'with', u'a', u'Linux', u'environment', u'and', u'shell', u'scripting', u'.']\n",
      "[u'Familiar', u'with', u'data', u'extract', u',', u'transform', u'and', u'load', u'processes', u'with', u'a', u'variety', u'of', u'data', u'types', u'.']\n",
      "[u'Strong', u'interpersonal', u'and', u'communication', u'skills', u'(', u'both', u'written', u'and', u'verbal', u')', u';', u'ability', u'to', u'communicate', u'with', u'people', u'in', u'a', u'wide', u'variety', u'of', u'areas', u'and', u'at', u'various', u'levels', u'from', u'technical', u'specialists', u'to', u'senior', u'management', u'.']\n",
      "[u'Ability', u'to', u'quickly', u'and', u'efficiently', u'adapt', u'to', u'new', u'concepts', u'.']\n",
      "[u'BSc', u'.', u'or', u'Masters', u'in', u'Mathematics', u',', u'Statistics', u'or', u'Computer', u'Science']\n",
      "[u'Nice-to-have']\n",
      "[u'Experience', u'with', u'Deep', u'Learning']\n",
      "[u'Passion', u'for', u'data', u'research', u',', u'algorithms', u'and', u'statistics']\n",
      "[]\n",
      "[u'What\\u2019s', u'in', u'it', u'for', u'you', u'?']\n",
      "[u'We', u'thrive', u'on', u'the', u'challenge', u'to', u'be', u'our', u'best', u',', u'progressive', u'thinking', u'to', u'keep', u'growing', u',', u'and', u'working', u'together', u'to', u'deliver', u'trusted', u'advice', u'to', u'help', u'our', u'clients', u'thrive', u'and', u'communities', u'prosper', u'.', u'We', u'care', u'about', u'each', u'other', u',', u'reaching', u'our', u'potential', u',', u'making', u'a', u'difference', u'to', u'our', u'communities', u',', u'and', u'achieving', u'success', u'that', u'is', u'mutual', u'.']\n",
      "[u'A', u'comprehensive', u'Total', u'Rewards', u'Program', u'including', u'bonuses', u'and', u'flexible', u'benefits', u',', u'competitive', u'compensation', u',', u'commissions', u',', u'and', u'stock', u'where', u'applicable']\n",
      "[u'Leaders', u'who', u'support', u'your', u'development', u'through', u'coaching', u'and', u'managing', u'opportunities']\n",
      "[u'Ability', u'to', u'make', u'a', u'difference', u'and', u'lasting', u'impact']\n",
      "[u'Work', u'in', u'a', u'dynamic', u',', u'collaborative', u',', u'progressive', u',', u'and', u'high-performing', u'team']\n",
      "[u'Opportunities', u'to', u'do', u'challenging', u'work']\n",
      "[]\n",
      "[u'Learn', u'more', u'about', u'RBC', u'Tech', u'Jobs']\n",
      "[]\n",
      "[u'About', u'RBC']\n",
      "[u'Royal', u'Bank', u'of', u'Canada', u'is', u'Canada\\u2019s', u'largest', u'bank', u',', u'and', u'one', u'of', u'the', u'largest', u'banks', u'in', u'the', u'world', u',', u'based', u'on', u'market', u'capitalization', u'.', u'We', u'are', u'one', u'of', u'North', u'America\\u2019s', u'leading', u'diversified', u'financial', u'services', u'companies', u',', u'and', u'provide', u'personal', u'and', u'commercial', u'banking', u',', u'wealth', u'management', u',', u'insurance', u',', u'investor', u'services', u'and', u'capital', u'markets', u'products', u'and', u'services', u'on', u'a', u'global', u'basis', u'.', u'We', u'have', u'over', u'80,000', u'full-', u'and', u'part-time', u'employees', u'who', u'serve', u'more', u'than', u'16', u'million', u'personal', u',', u'business', u',', u'public', u'sector', u'and', u'institutional', u'clients', u'through', u'offices', u'in', u'Canada', u',', u'the', u'U.S.', u'and', u'37', u'other', u'countries', u'.', u'For', u'more', u'information', u',', u'please', u'visit', u'rbc.com', u'.']\n",
      "[]\n",
      "[u'Join', u'our', u'Talent', u'Community']\n",
      "[u'Stay', u'in-the-know', u'about', u'great', u'career', u'opportunities', u'at', u'RBC', u'.', u'It\\u2019s', u'easy', u'!', u'Join', u'our', u'Talent', u'Community', u'and', u'get', u'the', u'inside', u'scoop', u'on', u'the', u'jobs', u',', u'career', u'paths', u',', u'and', u'recruitment', u'events', u'that', u'matter', u'to', u'you', u'.']\n",
      "[]\n",
      "[u'Inclusion', u'and', u'Equal', u'Opportunity', u'Employment']\n",
      "[u'RBC', u'is', u'an', u'equal', u'opportunity', u'employer', u'committed', u'to', u'diversity', u'and', u'inclusion', u'.', u'We', u'are', u'pleased', u'to', u'consider', u'all', u'qualified', u'applicants', u'for', u'employment', u'without', u'regard', u'to', u'race', u',', u'color', u',', u'religion', u',', u'sex', u',', u'sexual', u'orientation', u',', u'gender', u'identity', u',', u'national', u'origin', u',', u'age', u',', u'disability', u',', u'protected', u'veterans', u'status', u',', u'Aboriginal/Native', u'American', u'status', u'or', u'any', u'other', u'legally-protected', u'factors', u'.', u'Disability-related', u'accommodations', u'during', u'the', u'application', u'process', u'are', u'available', u'upon', u'request', u'.']\n",
      "[]\n",
      "[u'JOB', u'SUMMARY']\n",
      "[u'City', u':', u'Toronto']\n",
      "[u'Address', u':', u'88', u'Queens', u'Quay', u'W']\n",
      "[u'Work', u'Hours/Week', u':', u'37.5']\n",
      "[u'Work', u'Environment', u':', u'Office']\n",
      "[u'Employment', u'Type', u':', u'Permanent']\n",
      "[u'Career', u'Level', u':', u'Experienced', u'Hire/Professional']\n",
      "[u'Pay', u'Type', u':', u'Salaried']\n",
      "[u'Required', u'Travel', u'(', u'%', u')', u':', u'0']\n",
      "[u'Exempt/Non-Exempt', u':', u'N/A']\n",
      "[u'People', u'Manager', u':', u'No']\n",
      "[u'Job', u'Posting', u'End', u'Date', u':', u'02/28/2017']\n",
      "[u'Req', u'ID', u':', u'120697']\n",
      "[u'Posting', u'Notes', u':', u'None']\n",
      "[u'Job', u'Segment', u':']\n",
      "[u'Database', u',', u'Scientific', u',', u'Bank', u',', u'Banking', u',', u'Developer', u',', u'Engineering', u',', u'Technology', u',', u'Finance']\n"
     ]
    }
   ],
   "source": [
    "new_file = open(\"10-14-16-Backup/\" + \"RBC-Data_Scientist.txt\", \"r\")\n",
    "for item in new_file:\n",
    "    if not item == \"\\n\":\n",
    "        item = item.strip().decode('utf-8')\n",
    "        text = nltk.word_tokenize(item)\n",
    "        print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = S3Connection('AKIAJC44TMPWGMB3GF3Q', 'jNY1OXPuYQm4wVQ+RpA9oBu/Ta9H53PpAfRiJd0T')\n",
    "bucket = conn.get_bucket('indeed-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload local files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................."
     ]
    }
   ],
   "source": [
    "foldername = \"10-14-16-Backup/\"\n",
    "for filename in os.listdir(foldername):\n",
    "    #if filename.endswith(\".txt\") or filename.endswith(\".whatever\"):\n",
    "    name = foldername + filename\n",
    "    new_file = open(name, \"r\")\n",
    "    k = Key(bucket)\n",
    "    k.key = filename\n",
    "    k.set_contents_from_file(new_file, cb=percent_cb, num_cb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Key: indeed-parser,BMO_Financial_Group-Associate,_Client_Data_Managment.txt>\n",
      "<Key: indeed-parser,BMO_Financial_Group-Capital_Markets,_Business_Analyst_-_Business_Intelligence_Analyst.txt>\n",
      "<Key: indeed-parser,BMO_Financial_Group-Sr_Application_Software_Developer_-_(Lead_J2EE_SOAP_REST).txt>\n",
      "<Key: indeed-parser,RBC-Business_Intelligence_Analyst,_RBC_Insurance,_Mississauga.txt>\n",
      "<Key: indeed-parser,RBC-Data_Analyst_Winter_2017_Co-op_Opportunities.txt>\n",
      "<Key: indeed-parser,RBC-Data_Scientist.txt>\n",
      "<Key: indeed-parser,RBC-Data_Scientist_-_Machine_Learning.txt>\n",
      "<Key: indeed-parser,RBC-Machine_Learning_Researcher.txt>\n",
      "<Key: indeed-parser,RBC-Manager,_Digital_Product_Insight_Analyst.txt>\n",
      "<Key: indeed-parser,RBC-Operations_Analyst_(Data_Warehouse_Business_Intelligence).txt>\n",
      "<Key: indeed-parser,Scotiabank-Application_Development_Specialist_(DataPower\\SOA\\ESB).txt>\n",
      "<Key: indeed-parser,Scotiabank-Business_Analyst_WMTS_Data_Management.txt>\n",
      "<Key: indeed-parser,Scotiabank-Credit_Risk_Analyst_Portfolio_Oversight_Caribbean_&_Central_America.txt>\n",
      "<Key: indeed-parser,Scotiabank-Data_Scientist.txt>\n",
      "<Key: indeed-parser,Scotiabank-Manager_Quantitative_Analytics_and_Methodology.txt>\n",
      "<Key: indeed-parser,Scotiabank-Network_Analyst_Advisory_II.txt>\n",
      "<Key: indeed-parser,Scotiabank-Programmer_Analyst_Advisory.txt>\n",
      "<Key: indeed-parser,Scotiabank-Senior_Data_Scientist.txt>\n",
      "<Key: indeed-parser,Scotiabank-Statistician.txt>\n",
      "<Key: indeed-parser,Scotiabank-Technical_Product_Consultant_(3_openings).txt>\n",
      "<Key: indeed-parser,TD_Bank-Data_Scientist.txt>\n",
      "<Key: indeed-parser,TD_Bank-Environment_Management_Analyst.txt>\n",
      "<Key: indeed-parser,TD_Bank-Solution_Architect_-_Big_Data.txt>\n",
      "<Key: indeed-parser,TD_Bank-Solution_Developer_-_Big_Data.txt>\n"
     ]
    }
   ],
   "source": [
    "for obj in bucket.list():\n",
    "    print obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the content of k as string\n",
    "k.key = \"RBC-Machine_Learning_Researcher.txt\"\n",
    "k.get_contents_as_string(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Starting from the beginning, gensim’s word2vec expects a sequence of sentences as its input. \n",
    "Each sentence a list of words (utf8 strings)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
