{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Variables\"\"\"\n",
    "prefix_url = \"http://ca.indeed.com\"\n",
    "folder_name = \"Job-Posts-\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "token = \"Indeed.com\"\n",
    "created = False\n",
    "list_of_urls = []\n",
    "list_of_links = []\n",
    "list_of_banks = [\"RBC\", \"Scotiabank\", \"CIBC\", \"BMO Financial Group\", \"TD Bank\"]\n",
    "list_of_links.append(\"http://ca.indeed.com/jobs?q=data+science&l=Toronto%2C+ON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.secs = self.end - self.start\n",
    "        self.msecs = self.secs * 1000  # millisecs\n",
    "        if self.verbose:\n",
    "            print 'elapsed time: %f ms' % self.msecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def parse_next_url(url):\n",
    "    req = urllib2.request.Request(\n",
    "        url,\n",
    "        data=None,\n",
    "        headers={\n",
    "           'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "        }\n",
    "    )\n",
    "    response = urllib2.urlopen(req)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "def make_folder(directory):\n",
    "    #directory = directory + \"-\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    if not os.path.isdir(directory):\n",
    "        directory_path = os.path.join(os.getcwd(), directory)\n",
    "        os.makedirs(directory_path)\n",
    "        \n",
    "def save_job_description(info, text, url):\n",
    "    for key, val in info.iteritems():\n",
    "        file_name = ((key + \"-\" + val).replace(\" / \", \"_\").replace(\"/\", \"_\").replace(\" \", \"_\")) + \".txt\"\n",
    "        text_file = open(folder_name + \"/\" + file_name, \"w+\")\n",
    "        text_file.write(url.encode('utf-8'))\n",
    "        text_file.write(text)\n",
    "        text_file.close()\n",
    "    \n",
    "def parse_text(soup, type_of_company, title, url):\n",
    "\n",
    "    job_info = {}\n",
    "    complete_text = \"\"\n",
    "    \n",
    "    if is_it_indeed(soup):\n",
    "        job = soup.select('#job_summary')\n",
    "        raw_text = job[0].get_text().encode('utf-8')\n",
    "        complete_text = raw_text.strip()\n",
    "        \n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "        \n",
    "    if type_of_company == \"RBC\":\n",
    "        # Grabbing the raw job description text\n",
    "        job = soup.select(\".job\") # Entire job container\n",
    "        raw_text = job[0].get_text()\n",
    "        raw_text = raw_text.replace(u'\\xa0', u' ').encode('utf-8')\n",
    "        complete_text = raw_text.strip()\n",
    "        \n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "                \n",
    "    if type_of_company == \"Scotiabank\":\n",
    "        job = soup.select(\"#jobDesc p\") # Entire job container\n",
    "        if job:\n",
    "            for item in job:\n",
    "                complete_text += item.get_text()\n",
    "        else:\n",
    "            job = soup.select(\"#descriptiontext\") # Entire job container\n",
    "            if job:\n",
    "                for item in job:\n",
    "                    complete_text += item.get_text()\n",
    "            else:\n",
    "                job = soup.select(\"#requirementstext\") # Entire job container\n",
    "                for item in job:\n",
    "                    complete_text += item.get_text()\n",
    "                \n",
    "        complete_text = complete_text.encode('utf-8')\n",
    "        \n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "            \n",
    "    \n",
    "    if type_of_company == \"BMO Financial Group\":\n",
    "        job = soup.select('#initialHistory')\n",
    "        #print soup.prettify()\n",
    "        #print len(job)\n",
    "        complete_text = job[0]['value'].encode('utf-8')\n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "        \n",
    "    if type_of_company == \"TD Bank\":\n",
    "        job = soup.select(\"#tblJobDetail tr\")\n",
    "        #print len(job)\n",
    "        for a in job:\n",
    "            iteration = 0\n",
    "            row = a.select(\"td\")\n",
    "            for b in row:\n",
    "                if iteration == 0:\n",
    "                    complete_text += (b.get_text().strip() + \": \")\n",
    "                if iteration == 1:\n",
    "                    complete_text += b.get_text().strip()\n",
    "                iteration = iteration + 1\n",
    "            complete_text += \"\\n\"\n",
    "        complete_text = complete_text.encode('utf-8')\n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "    if type_of_company == \"CIBC\":\n",
    "        job = soup.select('#initialHistory')\n",
    "        complete_text = job[0]['value'].encode('utf-8')\n",
    "        job_info[type_of_company] = title\n",
    "        save_job_description(job_info, complete_text, url)\n",
    "    \n",
    "def create_paginations():\n",
    "    for i in range(190, 0, -10):\n",
    "        link = \"http://ca.indeed.com/jobs?q=data+science&l=Toronto%2C+ON&start=\" + str(i)\n",
    "        list_of_links.append(link)\n",
    "        \n",
    "def is_it_indeed(soup):\n",
    "    if not soup == None:\n",
    "        if not soup.find(\"title\") == None:\n",
    "            site_title = soup.find(\"title\").text\n",
    "        else:\n",
    "            site_title = \"\"\n",
    "    else:\n",
    "        print \"Soup is \" + soup\n",
    "        site_title = \"\"\n",
    "    return True if token in site_title else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parsing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer() as t:\n",
    "    make_folder(folder_name)\n",
    "    while list_of_links:\n",
    "        current_link = list_of_links.pop()\n",
    "        print \"Removing \" + current_link + \" from the list of links to scrape\"\n",
    "\n",
    "        soup = parse_next_url(current_link)\n",
    "\n",
    "        if not created:\n",
    "            create_paginations()\n",
    "            created = True\n",
    "\n",
    "        job_entry = soup.findAll(\"div\", class_=\"row\")\n",
    "\n",
    "        print \"There are \" + str(len(job_entry)) + \" job postings in this page\"\n",
    "\n",
    "        for job in job_entry:\n",
    "\n",
    "            company = job.find(\"span\", class_=\"company\")\n",
    "            job_info = job.find(\"h2\", class_=\"jobtitle\")\n",
    "            sponsored = job.find(\"span\", class_=\"sdn\")\n",
    "\n",
    "            #print job\n",
    "\n",
    "            if job_info is None: \n",
    "                # Sponsored\n",
    "                job_descr = job.find(\"a\", class_=\"jobtitle turnstileLink\")\n",
    "            else: \n",
    "                # Normal\n",
    "                job_descr = job_info.find(\"a\", class_=\"turnstileLink\")\n",
    "                \n",
    "            company_name = company.text.strip()\n",
    "\n",
    "            company_info = company.find(\"a\", class_=\"turnstileLink\")\n",
    "\n",
    "            job_title = job_descr.get(\"title\").strip()\n",
    "            job_href = job_descr.get(\"href\").strip()\n",
    "\n",
    "            final_url = prefix_url + job_href + \"\\n\"\n",
    "\n",
    "            # Add the url to a list for future crawl\n",
    "            list_of_urls.append({ 'company': company_name.encode('utf-8'), 'title': job_title.encode('utf-8'), 'url': final_url.encode('utf-8') })\n",
    "\n",
    "        for item in list_of_urls:\n",
    "            soup = parse_next_url(item['url'])\n",
    "            parse_text(soup, item['company'], item['title'], item['url'])\n",
    "            \n",
    "print \"=> elasped code: %s s\" % t.secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Example (not part of the flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_file = open(\"10-14-16-Backup/\" + \"RBC-Data_Scientist.txt\", \"r\")\n",
    "for item in new_file:\n",
    "    if not item == \"\\n\":\n",
    "        item = item.strip().decode('utf-8')\n",
    "        text = nltk.word_tokenize(item)\n",
    "        print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = S3Connection('AKIAJC44TMPWGMB3GF3Q', 'jNY1OXPuYQm4wVQ+RpA9oBu/Ta9H53PpAfRiJd0T')\n",
    "bucket = conn.get_bucket('indeed-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload local files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................."
     ]
    }
   ],
   "source": [
    "foldername = \"10-14-16-Backup/\"\n",
    "for filename in os.listdir(foldername):\n",
    "    #if filename.endswith(\".txt\") or filename.endswith(\".whatever\"):\n",
    "    name = foldername + filename\n",
    "    new_file = open(name, \"r\")\n",
    "    k = Key(bucket)\n",
    "    k.key = filename\n",
    "    k.set_contents_from_file(new_file, cb=percent_cb, num_cb=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data for Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Starting from the beginning, gensim’s word2vec expects a sequence of sentences as its input. \n",
    "Each sentence a list of words (utf8 strings)\n",
    "\"\"\"\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chensteven/anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/chensteven/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://ca.indeed.com/rc/clk?jk=e2a82a60924399d8&fccid=537b899e30af3338\n",
      ".\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/chensteven/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://ca.indeed.com/rc/clk?jk=7b6baff95bccb9b1&fccid=537b899e30af3338\n",
      ".\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/chensteven/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://ca.indeed.com/rc/clk?jk=f24a33c8ca8af736&fccid=537b899e30af3338\n",
      ".\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "num = 0\n",
    "for obj in bucket.list():\n",
    "    sentences += review_to_sentences(obj.get_contents_as_string(encoding=\"utf-8\"), tokenizer)\n",
    "    num = num + 1\n",
    "print num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture:\n",
    "skip gram vs cbow\n",
    "\n",
    "* Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "* Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "* Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "* Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "* Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "* Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "* Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'requirements', 0.9999343156814575),\n",
       " (u'learning', 0.999934196472168),\n",
       " (u'key', 0.9999340772628784),\n",
       " (u'required', 0.9999299049377441),\n",
       " (u'environment', 0.9999287128448486),\n",
       " (u'tools', 0.999927818775177),\n",
       " (u'models', 0.9999242424964905),\n",
       " (u'as', 0.9999219179153442),\n",
       " (u'statistical', 0.9999204277992249),\n",
       " (u'who', 0.9999188184738159)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'with', 0.9999685287475586),\n",
       " (u'development', 0.9999672770500183),\n",
       " (u'that', 0.9999610781669617),\n",
       " (u'provide', 0.9999576210975647),\n",
       " (u'knowledge', 0.9999572038650513),\n",
       " (u'analytics', 0.9999551773071289),\n",
       " (u'is', 0.999949038028717),\n",
       " (u'design', 0.999948263168335),\n",
       " (u'within', 0.9999452233314514),\n",
       " (u'work', 0.9999442100524902)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'understanding', 0.9998894929885864),\n",
       " (u'or', 0.9998721480369568),\n",
       " (u'sql', 0.9998646378517151),\n",
       " (u'code', 0.9998605251312256),\n",
       " (u'hadoop', 0.9998593926429749),\n",
       " (u'group', 0.9998583793640137),\n",
       " (u'issues', 0.9998553395271301),\n",
       " (u'competitive', 0.9998496174812317),\n",
       " (u'client', 0.9998486638069153),\n",
       " (u'communication', 0.9998477697372437)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
